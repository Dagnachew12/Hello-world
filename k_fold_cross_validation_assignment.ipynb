{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dagnachew12/Hello-world/blob/main/k_fold_cross_validation_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cousre : IE408_AI502_IE511\n",
        "### 2022.09.16\n",
        "\n"
      ],
      "metadata": {
        "id": "8PgZx_Sf_cur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model selection via K-fold cross validation\n",
        "\n",
        "## Table of Contents\n",
        "---\n",
        "- Custom dataset <br>\n",
        "- $K$-fold cross validation (Assigment)<br>\n",
        "- Custom activation function (Assigment)<br>\n",
        "- MLP construction<br>\n",
        "- Hyperparameter tuning<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "**`Assigment`** : Complete the two cell marked with (Assignment)\n",
        "- (Assignment) $K$-fold cross validation\n",
        "- (Assignment) Activation function with implementing forward and backward step\n",
        "- ðŸš¨ Please **do not modify** code that is not an Assigment cell\n",
        "- ðŸš¨ Please **do not add** any cells\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QGq5wOIZAQTY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import library"
      ],
      "metadata": {
        "id": "C9DDDg4PdeRB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PymY1HXxJV90"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "\n",
        "from torch.autograd import Function  # to create custom activation function\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Boston house prices dataset specification\n",
        "\n",
        "<img src = https://user-images.githubusercontent.com/43310063/188207759-db4dad3f-31e2-4fd2-9f71-7eaaf3f88329.png>\n",
        "\n",
        "### Data examples\n",
        "<img src = https://user-images.githubusercontent.com/43310063/188208483-f6dc9c50-3399-4607-af6b-cc3a444895e8.png>"
      ],
      "metadata": {
        "id": "su3fHNn4QjxS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom dataset in PyTorch\n",
        "### If you make *custom dataset*, you can use dataloader in pytorch\n",
        "<br>\n",
        "\n",
        "PyTorch `DATASETS` & `DATALOADERS` : <br>\n",
        "\n",
        "- `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` that allow you to use pre-loaded datasets as well as your own data. <br>\n",
        "- `Dataset` stores the samples and their corresponding labels.\n",
        "- `DataLoader` wraps an iterable around the Dataset to enable easy access to the samples.\n",
        "<br><br>\n",
        "\n",
        "- Creating a Custom Dataset for your files<br>\n",
        "    - A custom Dataset class must implement three functions: `__init__`, `__len__,` and `__getitem__`.\n",
        "    - The `__init__` function is run once when instantiating the Dataset object. We initialize the directory containing the images, the annotations file, and both transforms (covered in more detail in the next section).\n",
        "    - The `__len__` function returns the number of samples in our dataset.\n",
        "    - The `__getitem__` function loads and returns a sample from the dataset at the given index `idx`\n"
      ],
      "metadata": {
        "id": "9p21KQBwdo_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class BostonDataset(Dataset):  # from torch.utils.data import Dataset\n",
        "    def __init__(self, features, targets, train_mean=None, train_std=None):\n",
        "        self.features = torch.Tensor(features)\n",
        "        self.targets = torch.Tensor(targets).reshape(-1, 1)\n",
        "\n",
        "        # Standard Scaler using train_data's mean and train_data's std\n",
        "        if (train_mean is not None) and (train_std is not None):\n",
        "            self.features = (self.features - train_mean) / train_std\n",
        "\n",
        "    def __len__(self):  # return length of dataset\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, idx):  # return data with index(idx)\n",
        "        X = self.features[idx, :] \n",
        "        y = self.targets[idx]\n",
        "        \n",
        "        return X, y"
      ],
      "metadata": {
        "id": "UWgFc7HlwvTq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# $K$-fold cross validation\n",
        "\n",
        "- Lecture note has details (Week2-MLP)<br>\n",
        "\n",
        "- $K$-fold cross validation procedure\n",
        "    1. Divide the training dataset into k-parts\n",
        "    2. Use k-1 parts as training set and 1 part as validation set\n",
        "    3. Repeat the procedure K times, rotating the validation set\n",
        "    4. Average validation errors\n"
      ],
      "metadata": {
        "id": "WyyO6erTDtD6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Assignment) $K$-fold cross validation\n",
        "\n",
        "- Write your code in loops \n",
        "- Do not use any external library\n",
        "\n",
        "<br>\n",
        "\n",
        "return `rets` that is list <br> <br>\n",
        "`rets` form : \\\\\n",
        "ã€€ã€€ã€€ã€€[[train_index_list_1, validation_index_list_1], \\\\\n",
        "ã€€ã€€ã€€ã€€[train_index_list_2, validation_index_list_2], \\\\\n",
        "ã€€ã€€ã€€ã€€... \\\\\n",
        "ã€€ã€€ã€€ã€€[train_index_list_k, validation_index_list_k]] \\\\\n",
        "<br>\n",
        "len(`rets`) : k \\\\\n",
        "`rets`[0] : [train_index_list, validation_index_list]"
      ],
      "metadata": {
        "id": "nzW6JscQaZpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def k_fold_data(dataset, k):\n",
        "    rets = [] # list will have k-fold data, example rets[[train_index_list_1, validation_index_list_1], ... [train_index_list_k, validation_index_list_k]]\n",
        "    fold_size = len(dataset) // k\n",
        "    for i in range(k):\n",
        "        #### TODO : WRITE YOUR CODE IN THIS LOOP & COMPLETE k-fold ####\n",
        "        validation_idx = list(range(i*fold_size, (i+1)*fold_size))\n",
        "        train_idx = list(range(0,len(dataset)))\n",
        "        for j in range(fold_size):train_idx.remove(validation_idx[j])\n",
        "\n",
        "      \n",
        "  \n",
        "        \n",
        "        rets.append([train_idx, validation_idx])\n",
        "\n",
        "        # form of return variable :\n",
        "        # len(rets) : k \n",
        "        # rets[0] : [train_index_list, validation_index_list]\n",
        "        # example : \n",
        "        #   rets : [[[fold_size, ... len of dataset], [0, 1, .. fold_size-1],\n",
        "        #           [train_index_list_2, validation_index_list_2],\n",
        "        #           ...\n",
        "        #           [train_index_list_k, validation_index_list_k]]\n",
        "        #######################################################\n",
        "    return rets"
      ],
      "metadata": {
        "id": "ZjeuxS0_JWYz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load dataset & Split dataset"
      ],
      "metadata": {
        "id": "bZw4iuUiaGWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Boston house prices dataset\n",
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "\n",
        "X_data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "y_data = raw_df.values[1::2, 2]\n",
        "\n",
        "X_data = X_data.astype(np.float32)\n",
        "y_data = y_data.astype(np.float32)\n",
        "\n",
        "# Split the dataset into a training dataset and a test dataset\n",
        "test_size = 0.2\n",
        "train_len = int(X_data.shape[0] * (1-test_size))\n",
        "\n",
        "X_train, X_test = X_data[: train_len, :], X_data[train_len:, :]\n",
        "y_train, y_test = y_data[: train_len], y_data[train_len:]\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "BEelKv0wRwD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f2147c-c10d-4c03-bd67-544dd0a16817"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(404, 13) (102, 13) (404,) (102,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom activation function"
      ],
      "metadata": {
        "id": "fFXDJCke1OOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation function without trainable parameter\n",
        "- sigmoid activation function\n",
        "### $\\text{Sigmoid}(x) = \\sigma(x) = \\frac{1}{1+\\text{exp}(-x)}$"
      ],
      "metadata": {
        "id": "3IfbCvDU1SVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_(x):\n",
        "    return 1 / (1 + torch.exp(-x))\n",
        "\n",
        "class sigmoid(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return sigmoid_(x)"
      ],
      "metadata": {
        "id": "lTSS2Sdr1iF3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation function with implementing forward and backward step\n",
        "- tanh activation function\n",
        "### $\\text{tanh} = \\frac{\\text{exp}(x) - \\text{exp}(-x)}{\\text{exp}(x) + \\text{exp}(-x)}$\n",
        "\n"
      ],
      "metadata": {
        "id": "CvNdaCfW1SPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh_(x):\n",
        "    return (torch.exp(x) - torch.exp(-x)) / (torch.exp(x) + torch.exp(-x))\n",
        "\n",
        "class tanh(Function):  # from torch.autograd import Function\n",
        "    \n",
        "    @staticmethod  # python decorator\n",
        "    def forward(ctx, x):\n",
        "        ctx.save_for_backward(x)\n",
        "        \n",
        "        output = tanh_(x)\n",
        "        \n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        # At the top of backward unpack saved_tensors and initialize all gradients w.r.t. inputs to None.\n",
        "\n",
        "        x, = ctx.saved_tensors\n",
        "\n",
        "        if ctx.needs_input_grad[0]:\n",
        "            grad_input = 1 - tanh_(x) ** 2 # derivative of tanh(x) : 1 - tanh^2(x)\n",
        "\n",
        "        return grad_input * grad_output\n",
        "\n"
      ],
      "metadata": {
        "id": "4gyujK6r1it7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Assignment) Activation function with implementing forward and backward step\n",
        "\n",
        "- Write activation function, forward and backward step\n",
        "- Swish (paper : https://arxiv.org/abs/1710.05941)\n",
        "### $\\text{swish}(x) = x * \\text{Sigmoid}(\\beta x)$\n",
        "- $\\beta$ is a constant or trainable parameter\n",
        "- In this practice, $\\beta$ is fixed to 1"
      ],
      "metadata": {
        "id": "QPsOJexs1jRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class swish(Function):\n",
        "    \n",
        "    @staticmethod  # python decorator\n",
        "    def forward(ctx, x):\n",
        "        #### TODO : WRITE YOUR CODE ####\n",
        "        ctx.save_for_backward(x)\n",
        "        output = x*sigmoid_(x)\n",
        "\n",
        "        \n",
        "        return output\n",
        "        ################################\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        # At the top of backward unpack saved_tensors and initialize all gradients w.r.t. inputs to None.\n",
        "        #### TODO : WRITE YOUR CODE ####\n",
        "        x, = ctx.saved_tensors\n",
        "\n",
        "        if ctx.needs_input_grad[0]:\n",
        "          grad_input = x*sigmoid_(x) + x*sigmoid_(x)*(1 - sigmoid_(x))\n",
        "\n",
        "        return grad_input*grad_output # initialize all gradients w.r.t. inputs to None.\n",
        "        ################################"
      ],
      "metadata": {
        "id": "RjSJ5bOx1nKQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network model class"
      ],
      "metadata": {
        "id": "MuDwc0ZOdxJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_feature, n_hidden, n_output, dropout_rate=0.2):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(n_feature, n_hidden)\n",
        "        self.a1 = swish.apply  # activation function we implemented\n",
        "        \n",
        "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
        "        self.a2 = swish.apply\n",
        "\n",
        "        self.fc3 = nn.Linear(n_hidden, n_output)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate) # dropout with dropout_rate\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.a1(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.a2(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "n-onkZDOLdCQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training \n",
        "- Training with $K$-fold\n",
        "- Hyperparameter optimization (HPO)"
      ],
      "metadata": {
        "id": "LuDS5iU4fby2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "# training hyperparameters used to HPO\n",
        "hpo_vals = {  # our search space for HPO\n",
        "    'learning_rate': [0.001, 0.005, 0.01],\n",
        "    'weight_decay' : [0.7, 0.8, 0.9],\n",
        "    'dropout_rate' : [0.1, 0.2, 0.3],\n",
        "}\n",
        "hp_candidates = []\n",
        "\n",
        "items = sorted(hpo_vals.items())\n",
        "keys, vals = zip(*items)\n",
        "\n",
        "# product do cartesian product, so it creates all combinations\n",
        "for v in product(*vals):  \n",
        "    hp_candidates.append(dict(zip(keys, v)))\n",
        "\n",
        "print(\"Length of hyperparameter candidates :\", len(hp_candidates))\n",
        "print(hp_candidates)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5u0OCtce1s3V",
        "outputId": "7b185107-ffb3-49bf-9da1-c9fc37bf08b5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of hyperparameter candidates : 27\n",
            "[{'dropout_rate': 0.1, 'learning_rate': 0.001, 'weight_decay': 0.7}, {'dropout_rate': 0.1, 'learning_rate': 0.001, 'weight_decay': 0.8}, {'dropout_rate': 0.1, 'learning_rate': 0.001, 'weight_decay': 0.9}, {'dropout_rate': 0.1, 'learning_rate': 0.005, 'weight_decay': 0.7}, {'dropout_rate': 0.1, 'learning_rate': 0.005, 'weight_decay': 0.8}, {'dropout_rate': 0.1, 'learning_rate': 0.005, 'weight_decay': 0.9}, {'dropout_rate': 0.1, 'learning_rate': 0.01, 'weight_decay': 0.7}, {'dropout_rate': 0.1, 'learning_rate': 0.01, 'weight_decay': 0.8}, {'dropout_rate': 0.1, 'learning_rate': 0.01, 'weight_decay': 0.9}, {'dropout_rate': 0.2, 'learning_rate': 0.001, 'weight_decay': 0.7}, {'dropout_rate': 0.2, 'learning_rate': 0.001, 'weight_decay': 0.8}, {'dropout_rate': 0.2, 'learning_rate': 0.001, 'weight_decay': 0.9}, {'dropout_rate': 0.2, 'learning_rate': 0.005, 'weight_decay': 0.7}, {'dropout_rate': 0.2, 'learning_rate': 0.005, 'weight_decay': 0.8}, {'dropout_rate': 0.2, 'learning_rate': 0.005, 'weight_decay': 0.9}, {'dropout_rate': 0.2, 'learning_rate': 0.01, 'weight_decay': 0.7}, {'dropout_rate': 0.2, 'learning_rate': 0.01, 'weight_decay': 0.8}, {'dropout_rate': 0.2, 'learning_rate': 0.01, 'weight_decay': 0.9}, {'dropout_rate': 0.3, 'learning_rate': 0.001, 'weight_decay': 0.7}, {'dropout_rate': 0.3, 'learning_rate': 0.001, 'weight_decay': 0.8}, {'dropout_rate': 0.3, 'learning_rate': 0.001, 'weight_decay': 0.9}, {'dropout_rate': 0.3, 'learning_rate': 0.005, 'weight_decay': 0.7}, {'dropout_rate': 0.3, 'learning_rate': 0.005, 'weight_decay': 0.8}, {'dropout_rate': 0.3, 'learning_rate': 0.005, 'weight_decay': 0.9}, {'dropout_rate': 0.3, 'learning_rate': 0.01, 'weight_decay': 0.7}, {'dropout_rate': 0.3, 'learning_rate': 0.01, 'weight_decay': 0.8}, {'dropout_rate': 0.3, 'learning_rate': 0.01, 'weight_decay': 0.9}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# model hyperparameters\n",
        "n_features = X_train.shape[1] # the number of features in input\n",
        "n_hidden = 256\n",
        "n_output = 1\n",
        "\n",
        "# training hyperparameters\n",
        "batch_size = 32\n",
        "num_epochs = 20\n",
        "k_folds = 5\n",
        "\n",
        "hpo_results = []\n",
        "\n",
        "# for standard scaler\n",
        "train_mean = X_train.mean(axis=0)  # mean for each features\n",
        "train_std = X_train.std(axis=0)  # stdandard deviation for each features\n",
        "\n",
        "train_dataset = BostonDataset(X_train, y_train, train_mean, train_std)\n",
        "test_dataset = BostonDataset(X_test, y_test, train_mean, train_std)\n",
        "\n",
        "# Loops for each hyper-parameter combination\n",
        "# Train model using k-fold with a hyper-parameter combination\n",
        "for hp in hp_candidates:\n",
        "\n",
        "    # Load hyper-parameters for training\n",
        "    learning_rate = hp[\"learning_rate\"]\n",
        "    weight_decay = hp[\"weight_decay\"]\n",
        "    dropout_rate = hp[\"dropout_rate\"]\n",
        "\n",
        "    validation_logs = [[] for _ in range(k_folds)]\n",
        "\n",
        "    # Loops for each fold\n",
        "    for fold_idx, data_idx in enumerate(k_fold_data(train_dataset, k_folds)):\n",
        "        model = MLP(n_features, n_hidden, n_output, dropout_rate).to(device) # new MLP model for each fold\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay) # use weight decay to prevent overfitting\n",
        "        criterion = nn.MSELoss() # use Mean Squared Error, because it is a regression problem that predicts house prices\n",
        "\n",
        "        train_idx, validation_idx = data_idx  # results of k_fold_data function\n",
        "\n",
        "        # divide train_dataset into train_subtset and validation_subset using the indexes that are results of k_fold_data function\n",
        "        train_subset = Subset(train_dataset, train_idx)\n",
        "        validation_subset = Subset(train_dataset, validation_idx)\n",
        "\n",
        "        train_dataloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "        validation_dataloader = DataLoader(validation_subset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        validation_losses = []\n",
        "\n",
        "        model.train()\n",
        "        for epoch in range(num_epochs):\n",
        "            train_loss = 0.\n",
        "            for i, (inputs, targets) in enumerate(train_dataloader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "                optimizer.zero_grad() \n",
        "                # sets the gradients of all optimized torch.Tensors to zero before starting to do backpropagation\n",
        "                # By default, PyTorch accumulates the gradients. Accumulating process is convinient while training RNN\n",
        "\n",
        "                y_pred = model(inputs)\n",
        "\n",
        "                loss = criterion(y_pred, targets)\n",
        "                loss.backward()  # Computes the gradient of current tensor\n",
        "                optimizer.step()  # Performs a single optimization step (parameter update)\n",
        "\n",
        "                train_loss += loss.item() * inputs.shape[0]\n",
        "\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                validation_loss = 0.\n",
        "                for i, (inputs, targets) in enumerate(validation_dataloader):\n",
        "                    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "                    y_pred = model(inputs)\n",
        "\n",
        "                    validation_loss += criterion(y_pred, targets).item() * inputs.shape[0]\n",
        "\n",
        "                validation_logs[fold_idx].append(validation_loss / len(validation_subset))\n",
        "\n",
        "    # Validation score is calculated by averaging the results of each folds\n",
        "    validation_score = np.mean(validation_logs)\n",
        "\n",
        "    hpo_results.append([*hp.values(), validation_score])\n"
      ],
      "metadata": {
        "id": "2NpiPTvK11KX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter tuning results\n",
        "\n",
        "- Low validation score is better in our practice, because we use the Mean Squared Error"
      ],
      "metadata": {
        "id": "vEOruxMVggkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = [*keys, \"validation_score\"]\n",
        "hpo_df = pd.DataFrame(hpo_results, columns=cols)\n",
        "hpo_sorted_df = hpo_df.sort_values(by=\"validation_score\")  # sort validation socre in ascending order because we used mean squared error\n",
        "hpo_sorted_df\n",
        "\n",
        "# Lower validation score is better\n",
        "# In our search space, the top row of dataframe is the best hyperparameter combination"
      ],
      "metadata": {
        "id": "3UIEHVOWFY_t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "outputId": "b621f40b-0425-4ac2-f19e-56ebd3ae912b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    dropout_rate  learning_rate  weight_decay  validation_score\n",
              "7            0.1          0.010           0.8         50.407188\n",
              "6            0.1          0.010           0.7         51.517361\n",
              "8            0.1          0.010           0.9         55.111931\n",
              "26           0.3          0.010           0.9         55.230419\n",
              "16           0.2          0.010           0.8         57.227335\n",
              "25           0.3          0.010           0.8         58.302993\n",
              "17           0.2          0.010           0.9         61.017113\n",
              "15           0.2          0.010           0.7         62.392601\n",
              "24           0.3          0.010           0.7         62.538395\n",
              "3            0.1          0.005           0.7         78.245978\n",
              "21           0.3          0.005           0.7         79.440312\n",
              "4            0.1          0.005           0.8         79.772984\n",
              "23           0.3          0.005           0.9         80.292779\n",
              "13           0.2          0.005           0.8         80.327356\n",
              "12           0.2          0.005           0.7         81.144953\n",
              "22           0.3          0.005           0.8         84.616160\n",
              "14           0.2          0.005           0.9         86.848300\n",
              "5            0.1          0.005           0.9         89.848189\n",
              "1            0.1          0.001           0.8        227.398971\n",
              "2            0.1          0.001           0.9        229.736053\n",
              "0            0.1          0.001           0.7        230.460403\n",
              "18           0.3          0.001           0.7        232.038554\n",
              "9            0.2          0.001           0.7        233.341818\n",
              "10           0.2          0.001           0.8        235.564299\n",
              "20           0.3          0.001           0.9        237.372195\n",
              "11           0.2          0.001           0.9        239.653733\n",
              "19           0.3          0.001           0.8        242.831947"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-642ce150-f444-4ef6-8227-797c9db03f68\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dropout_rate</th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>weight_decay</th>\n",
              "      <th>validation_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.8</td>\n",
              "      <td>50.407188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.7</td>\n",
              "      <td>51.517361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.9</td>\n",
              "      <td>55.111931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.9</td>\n",
              "      <td>55.230419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.8</td>\n",
              "      <td>57.227335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.8</td>\n",
              "      <td>58.302993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.9</td>\n",
              "      <td>61.017113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.7</td>\n",
              "      <td>62.392601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.7</td>\n",
              "      <td>62.538395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.7</td>\n",
              "      <td>78.245978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.7</td>\n",
              "      <td>79.440312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.8</td>\n",
              "      <td>79.772984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.9</td>\n",
              "      <td>80.292779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.8</td>\n",
              "      <td>80.327356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.7</td>\n",
              "      <td>81.144953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.8</td>\n",
              "      <td>84.616160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.9</td>\n",
              "      <td>86.848300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.9</td>\n",
              "      <td>89.848189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.8</td>\n",
              "      <td>227.398971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.9</td>\n",
              "      <td>229.736053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.7</td>\n",
              "      <td>230.460403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.7</td>\n",
              "      <td>232.038554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.7</td>\n",
              "      <td>233.341818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.8</td>\n",
              "      <td>235.564299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.9</td>\n",
              "      <td>237.372195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.9</td>\n",
              "      <td>239.653733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.8</td>\n",
              "      <td>242.831947</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-642ce150-f444-4ef6-8227-797c9db03f68')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-642ce150-f444-4ef6-8227-797c9db03f68 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-642ce150-f444-4ef6-8227-797c9db03f68');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Test model with best hyperparameter setting"
      ],
      "metadata": {
        "id": "b--FifHX2CLh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model with best hyperparameter using full training dataset without $K$-fold"
      ],
      "metadata": {
        "id": "7GoO0rvY2Gqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "Em7sd0NGg7-k"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for plotting loss\n",
        "def plot_loss(loss_list):\n",
        "    clear_output(True) # clear output in executing cell\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.ylabel(\"Train Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.plot(loss_list)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "fTwA5nl3fQ9r"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training hyperparameters\n",
        "batch_size = 32\n",
        "num_epochs = 100\n",
        "\n",
        "# Use best hyperparameter setting, The top row of dataframe is the best hyperparameter\n",
        "learning_rate = hpo_sorted_df.iloc[0].learning_rate\n",
        "weight_decay = hpo_sorted_df.iloc[0].weight_decay\n",
        "dropout_rate = hpo_sorted_df.iloc[0].dropout_rate\n",
        "\n",
        "model_with_hpo = MLP(n_features, n_hidden, n_output, dropout_rate).to(device)\n",
        "optimizer = torch.optim.Adam(model_with_hpo.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "criterion = nn.MSELoss() # using Mean Squared Error\n",
        "\n",
        "# use full training dataset without K-fold\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "train_logs = []\n",
        "\n",
        "model_with_hpo.train()\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.\n",
        "    for i, (inputs, targets) in enumerate(train_dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_pred = model_with_hpo(inputs)\n",
        "\n",
        "        loss = criterion(y_pred, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * inputs.shape[0]\n",
        "\n",
        "    train_logs.append(train_loss / len(train_dataset))\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "        plot_loss(train_logs)\n",
        "        "
      ],
      "metadata": {
        "id": "ZuTeMpkq2K-L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "b3cce9e5-d92f-4782-ed5c-4407b8f31a97"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_logs[-1], np.array(train_logs).min()"
      ],
      "metadata": {
        "id": "Q3MpdrsAXtLl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e3c23e2-0ec9-4989-9f35-9e0b3410a333"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17.905517880279238, 13.364932971425576)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test model with best hyperparameter"
      ],
      "metadata": {
        "id": "n1iSnBou2Gnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_logs = []\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "criterion = nn.MSELoss(reduction='sum') # sum loss using Mean Squared Error\n",
        "\n",
        "model_with_hpo.eval()\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, targets) in enumerate(test_dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        y_pred = model_with_hpo(inputs)\n",
        "        test_logs.append(criterion(y_pred, targets).item())\n",
        "\n",
        "print(f'average test log mse = {sum(test_logs)/ len(test_dataset)}')"
      ],
      "metadata": {
        "id": "icmjnbgF2Lfm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18dbd6bd-2e59-4196-877d-ad41615c7e56"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average test log mse = 20.322744182511872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html\n",
        "- https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "- https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "- https://github.com/scikit-learn/scikit-learn/blob/36958fb24/sklearn/model_selection/_search.py#L1021\n",
        "- https://towardsdatascience.com/extending-pytorch-with-custom-activation-functions-2d8b065ef2fa\n",
        "- https://pytorch.org/docs/stable/notes/extending.html"
      ],
      "metadata": {
        "id": "GL-DlCXu6gnC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test part\n",
        "***Do not change*** this part and ***Do not add*** cell below this part<br>\n",
        "- This part is not for students\n",
        "- Do not use & add below cell"
      ],
      "metadata": {
        "id": "d1nALDNewqHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import gradcheck\n",
        "\n",
        "input = (torch.rand(32, 13, dtype=torch.float, requires_grad=True)).to(device)\n",
        "\n",
        "test = gradcheck(model_with_hpo, input, eps=1e-3, atol=1e-2)"
      ],
      "metadata": {
        "id": "cOc667_fvKt2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "outputId": "db321efd-9727-4429-97c8-1da5e384bdf1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/autograd/gradcheck.py:653: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. \n",
            "  f'Input #{idx} requires gradient and '\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "GradcheckError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mGradcheckError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-f959a6ca6f45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_with_hpo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/gradcheck.py\u001b[0m in \u001b[0;36mgradcheck\u001b[0;34m(func, inputs, eps, atol, rtol, raise_exception, check_sparse_nnz, nondet_tol, check_undefined_grad, check_grad_dtypes, check_batched_grad, check_batched_forward_grad, check_forward_ad, check_backward_ad, fast_mode)\u001b[0m\n\u001b[1;32m   1412\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_gradcheck_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/gradcheck.py\u001b[0m in \u001b[0;36m_gradcheck_helper\u001b[0;34m(func, inputs, eps, atol, rtol, check_sparse_nnz, nondet_tol, check_undefined_grad, check_grad_dtypes, check_batched_grad, check_batched_forward_grad, check_forward_ad, check_backward_ad, fast_mode)\u001b[0m\n\u001b[1;32m   1429\u001b[0m                          \u001b[0mrtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_grad_dtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_forward_ad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_forward_ad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m                          \u001b[0mcheck_backward_ad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_backward_ad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnondet_tol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnondet_tol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1431\u001b[0;31m                          check_undefined_grad=check_undefined_grad)\n\u001b[0m\u001b[1;32m   1432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcheck_batched_forward_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/gradcheck.py\u001b[0m in \u001b[0;36m_gradcheck_real_imag\u001b[0;34m(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps, rtol, atol, check_grad_dtypes, check_forward_ad, check_backward_ad, nondet_tol, check_undefined_grad)\u001b[0m\n\u001b[1;32m   1074\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m             gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,\n\u001b[0;32m-> 1076\u001b[0;31m                          rtol, atol, check_grad_dtypes, nondet_tol)\n\u001b[0m\u001b[1;32m   1077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcheck_forward_ad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/gradcheck.py\u001b[0m in \u001b[0;36m_slow_gradcheck\u001b[0;34m(func, func_out, tupled_inputs, outputs, eps, rtol, atol, check_grad_dtypes, nondet_tol, use_forward_ad, complex_indices, test_imag)\u001b[0m\n\u001b[1;32m   1126\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalytical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumerical\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_allclose_with_type_promotion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1128\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mGradcheckError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_notallclose_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomplex_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_imag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mGradcheckError\u001b[0m: Jacobian mismatch for output 0 with respect to input 0,\nnumerical:tensor([[-3.4873,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [-0.6506,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [-0.4373,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        ...,\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -1.5144],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.1822],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -5.2099]],\n       device='cuda:0')\nanalytical:tensor([[-1.9873,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0144,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.2989,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        ...,\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.7021],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -1.1633],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -2.7319]],\n       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test)"
      ],
      "metadata": {
        "id": "UVphOEKQwb1U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e8345f5-d020-458f-afd8-67116029fa02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(test_logs)/ len(test_dataset))"
      ],
      "metadata": {
        "id": "4tbP_QJEw32O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09bc42ce-9771-4bf2-ae89-7548f0ca2b7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19.304763419955385\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k_test = 5\n",
        "k_fold_test = k_fold_data(input, k_test)\n",
        "\n",
        "check = True\n",
        "\n",
        "if len(k_fold_test) != k_test:    \n",
        "    check = False\n",
        "else:\n",
        "    for fold in k_fold_test:\n",
        "        if len(fold[1]) != (input.shape[0] // k_test):\n",
        "            check = False\n",
        "            break\n",
        "\n",
        "print(check)"
      ],
      "metadata": {
        "id": "dBFs_cHUa-_Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1abb33af-e85c-42e2-ca51-b827c206d1c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    }
  ]
}